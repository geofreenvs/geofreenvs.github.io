<!Doctype html>
<html lang="en">
    <head>
        <title>AI3DCC: Workshop on AI for 3D Content Creation</title>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="author" content="Despoina Paschalidou">
        <meta name="description" content="Workshop page for ai3dcc">
        <!-- Bootstrap -->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" type="text/css" href="style.css?cache=77333914184988801948">
        <link rel="icon" type="image/png" href="figures/favicon.png"/>
    </head>
    <body>
        <div id="navbar-top">
        <nav class="navbar navbar-light px-3">
            <a class="navbar-brand" href="https://iccv2023.thecvf.com/">AI3DCC @ ICCV2023</a>
            <ul class="nav nav-pills">
              <li class="nav-item">
                <a class="nav-link active" href="index.html#home">Home</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#submission">Submission</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#schedule">Schedule</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="accepted_papers.html">
                Accepted Papers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#speakers">Speakers</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="program_committee.html">
                Program Committee</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="index.html#organizers">Organizers</a>
              </li>
            </ul>
        </nav>
        </div>
        <div data-bs-spy="scroll" data-bs-target="#navbar-example2" data-bs-offset="0" class="scrollspy-example" tabindex="0">

        <div class="jumbotron">
            <div class="papers-title">
                <h2>Archival Workshop Publications</h2>
            </div>
            <!-- start paper list --><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2309.07917">Looking at Words and Points with Attention: a Benchmark for Text-to-Shape Coherence</a></div><div class="authors">Andrea Amaduzzi; Giuseppe Lisanti; Samuele Salti; Luigi Di Stefano</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2309.07917.pdf" data-type="Paper">Paper</a> <a href="posters/full_poster_Andrea_Amaduzzi.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">While text-conditional 3D object generation and manipulation have seen rapid progress, the evaluation of coherence between generated 3D shapes and input textual descriptions lacks a clear benchmark. The reason is twofold: a) the low quality of the textual descriptions in the only publicly available dataset of text-shapes pairs; b) the limited effectiveness of the metrics used to quantitatively assess such coherence. In this paper, we propose a comprehensive solution that addresses both weaknesses. Firstly, we employ large language models to automatically refine textual descriptions associated with shapes. Secondly, we propose a quantitative metric to assess text-to-shape coherence, through cross-attention mechanism. To validate our approach, we conduct a human study and compare quantitatively our metric with existing ones. The refined dataset, the new metric and a set of text-shape pairs validated by the human study comprise a novel, fine-grained benchmark that we publicly release to foster research on text-to-shape coherence of text-conditioned 3D generative models.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://www.lix.polytechnique.fr/vista/projects/2023_iccvw_courant/">BluNF: Blueprint Neural Field</a></div><div class="authors">Robin Courant; Xi WANG; Marc Christie; Vicky Kalogeiton</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2309.03933.pdf" data-type="Paper">Paper</a> <a href="posters/BluNF-poster.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Neural Radiance Fields (NeRFs) have revolutionized scene novel view synthesis, offering visually realistic, precise, and robust implicit reconstructions. While recent approaches enable NeRF editing, such as object removal, 3D shape modification, or material property manipulation, the manual annotation prior to such edits makes the process tedious. Additionally, traditional 2D interaction tools lack an accurate sense of 3D space, preventing precise manipulation and editing of scenes. In this paper, we introduce a novel approach, called Blueprint Neural Field (BluNF), to address these editing issues. BluNF provides a robust and user-friendly 2D blueprint, enabling intuitive scene editing. By leveraging implicit neural representation, BluNF constructs a blueprint of a scene using prior semantic and depth information. The generated blueprint allows effortless editing and manipulation of NeRF representations. We demonstrate BluNF's editability through an intuitive click-and-change mechanism, enabling 3D manipulations, such as masking, appearance modification, and object removal. Our approach significantly contributes to visual content creation, paving the way for further research in this area.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2303.12865">NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions</a></div><div class="authors">Shahbazi Mohamad; Ntavelis Evangelos; Tonioni Alessio; Collins Edo; Paudel Danda Pani; Danelljan Martin; Van Gool Luc</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2303.12865.pdf" data-type="Paper">Paper</a> <a href="posters/ICCVW2023_NeRF-GAN_Poster.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of neural 3D representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANs. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed method obtains results comparable with volumetric rendering in terms of quality and 3D consistency while benefiting from the computational advantage of convolutional networks.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2212.01381">LatentSwap3D: Semantic Edits on 3D Image GANs</a></div><div class="authors">Enis Simsar; Alessio Tonioni; Evin Pınar Örnek; Federico Tombari</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2212.01381.pdf" data-type="Paper">Paper</a> <a href="posters/AI3DCC-LatentSwap3D.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Recent 3D GANs have the ability to generate latent codes for entire 3D volumes rather than only 2D images. While they offer desirable features like high-quality geometry and multi-view consistency, complex semantic image editing tasks for 3D GANs have only been partially explored, unlike their 2D counterparts, e.g., StyleGAN and its variants. To address this problem, we propose LatentSwap3D, a latent space discovery-based semantic edit approach which can be used with any off-the-shelf 3D or 2D GAN model and on any dataset. LatentSwap3D relies on identifying the latent code dimensions corresponding to specific attributes by feature ranking of a random forest classifier. It then performs the edit by swapping the selected dimensions of the image being edited with the ones from an automatically selected reference image. Compared to other latent space control-based edit methods, which were mainly designed for 2D GANs, our method on 3D GANs provides remarkably consistent semantic edits in a disentangled manner and outperforms others both qualitatively and quantitatively. We show results on seven 3D generative models (pi-GAN, GIRAFFE, StyleSDF, MVCGAN, EG3D, StyleNeRF, and VolumeGAN) and on five datasets (FFHQ, AFHQ, Cats, MetFaces, and CompCars).</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2309.00158">BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models</a></div><div class="authors">Yao Wei; George Vosselman; Michael Ying Yang</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2309.00158.pdf" data-type="Paper">Paper</a> <a href="posters/ICCV23_AI3DCC_BuilDiff.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">3D building generation with low data acquisition costs, such as single image-to-3D, becomes increasingly important. However, most of the existing single image-to-3D building creation works are restricted to those images with specific viewing angles, hence they are difficult to scale to general-view images that commonly appear in practical cases. To fill this gap, we propose a novel 3D building shape generation method exploiting point cloud diffusion models with image conditioning schemes, which demonstrates flexibility to the input images. By cooperating two conditional diffusion models and introducing a regularization strategy during denoising process, our method is able to synthesize building roofs while maintaining the overall structures. We validate our framework on two newly built datasets and extensive experiments show that our method outperforms previous works in terms of building generation quality.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2303.13450">Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes</a></div><div class="authors">Dana Cohen-Bar; Elad Richardson; Gal Metzer; Raja Giryes; Danny Cohen-Or</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2303.13450.pdf" data-type="Paper">Paper</a> <a href="posters/set_the_scene.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Recent breakthroughs in text-guided image generation have led to remarkable progress in the field of 3D synthesis from text. By optimizing neural radiance fields (NeRF) directly from text, recent methods are able to produce remarkable results. Yet, these methods are limited in their control of each object's placement or appearance, as they represent the scene as a whole. This can be a major issue in scenarios that require refining or manipulating objects in the scene. To remedy this deficit, we propose a novel Global-Local training framework for synthesizing a 3D scene using object proxies. A proxy represents the object's placement in the generated scene and optionally defines its coarse geometry. The key to our approach is to represent each object as an independent NeRF. We alternate between optimizing each NeRF on its own and as part of the full scene. Thus, a complete representation of each object can be learned, while also creating a harmonious scene with style and lighting match. We show that using proxies allows a wide variety of editing options, such as adjusting the placement of each independent object, removing objects from a scene, or refining an object. Our results show that Set-the-Scene offers a powerful solution for scene synthesis and manipulation, filling a crucial gap in controllable text-to-3D synthesis.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://abdullahamdi.com/sparf/">SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images</a></div><div class="authors">Abdullah J Hamdi; Bernard Ghanem; Matthias Niessner</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2212.09100.pdf" data-type="Paper">Paper</a> <a href="posters/SPARF_poster.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (plenoxels,INGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of ~ 17 million images rendered from nearly 40,000 shapes at high resolution (400 * 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to generate high-quality sparse voxel radiance fields that can be rendered from novel views. Our approach achieves state-of-the-art results in the task of unconstrained novel view synthesis based on few views on ShapeNet as compared to recent baselines. The SPARF dataset will be made publicly available with the code and models upon publication.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://www.vision.huji.ac.il/blended-nerf/">Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields</a></div><div class="authors">Ori Gordon; Omri Avrahami; Dani Lischinski</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2306.12760.pdf" data-type="Paper">Paper</a> <a href="posters/BlendedNeRF.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Editing a local region or a specific object in a 3D scene represented by a NeRF or consistently blending a new realistic object into the scene is challenging, mainly due to the implicit nature of the scene representation. We present Blended-NeRF, a robust and flexible framework for editing a specific region of interest in an existing NeRF scene, based on text prompts, along with a 3D ROI box. Our method leverages a pretrained language-image model to steer the synthesis towards a user-provided text prompt, along with a 3D MLP model initialized on an existing NeRF scene to generate the object and blend it into a specified region in the original scene. We allow local editing by localizing a 3D ROI box in the input scene, and blend the content synthesized inside the ROI with the existing scene using a novel volumetric blending technique. To obtain natural looking and view-consistent results, we leverage existing and new geometric priors and 3D augmentations for improving the visual fidelity of the final result. We test our framework both qualitatively and quantitatively on a variety of real 3D scenes and text prompts, demonstrating realistic multi-view consistent results with much flexibility and diversity compared to the baselines. Finally, we show the applicability of our framework for several 3D editing applications, including adding new objects to a scene, removing/replacing/altering existing objects, and texture conversion.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2309.01252">S2RF: Semantically Stylized Radiance Fields</a></div><div class="authors">Moneish Kumar; Neeraj Panse; Dishani Lahiri</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2309.01252.pdf" data-type="Paper">Paper</a> <a href="posters/ICCV-Poster-S2RF.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We present our method for transferring style from any arbitrary image(s) to object(s) within a 3D scene. Our primary objective is to offer more control in 3D scene stylization, facilitating the creation of customizable and stylized scene images from arbitrary viewpoints. To achieve this, we propose a novel approach that incorporates nearest neighborhood-based loss, allowing for flexible 3D scene reconstruction while effectively capturing intricate style details and ensuring multi-view consistency.</div></div></div></div><!-- end paper list -->
            <div class="papers-title">
                <h2>Non-Archival Workshop Publications</h2>
            </div>
            <!-- start non-arc paper list --><div class="row paper"><div class="content"><div class="paper-title"><a href="https://ray-cond.github.io/">Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation</a></div><div class="authors">Eric M Chen; Sidhanth Holalkere; Ruyu Yan; Kai Zhang; Abe Davis</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2304.13681.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">Multi-view image generation attracts particular attention these days due to its promising 3D-related applications, e.g., image viewpoint editing. Most existing methods follow a paradigm where a 3D representation is first synthesized, and then rendered into 2D images to ensure photo-consistency across viewpoints. However, such explicit bias for photo-consistency sacrifices photo-realism, causing geometry artifacts and loss of fine-scale details when these methods are applied to edit real images. To address this issue, we propose ray conditioning, a geometry-free alternative that relaxes the photo-consistency constraint. Our method generates multi-view images by conditioning a 2D GAN on a light field prior. With explicit viewpoint control, state-of-the-art photo-realism and identity consistency, our method is particularly suited for the viewpoint editing task.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://snap-research.github.io/3DVADER/">Autodecoding Latent 3D Diffusion Models</a></div><div class="authors">Evangelos Ntavelis; Aliaksandr Siarohin; Kyle B Olszewski; Chaoyang Wang; Luc Van Gool; Sergey Tulyakov</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2307.05445.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">We present a novel approach to the generation of static and articulated 3D assets that has a 3D autodecoder at its core. The 3D autodecoder framework embeds properties learned from the target dataset in the latent space, which can then be decoded into a volumetric representation for rendering view-consistent appearance and geometry. We then identify the appropriate intermediate volumetric latent space, and introduce robust normalization and de-normalization operations to learn a 3D diffusion from 2D images or monocular videos of rigid or articulated objects. Our approach is flexible enough to use either existing camera supervision or no camera information at all -- instead efficiently learning it during training. Our evaluations demonstrate that our generation results outperform state-of-the-art alternatives on various benchmark datasets and metrics, including multi-view image datasets of synthetic objects, real in-the-wild videos of moving people, and a large-scale, real video dataset of static objects.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://lukashoel.github.io/text-to-room/">Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models</a></div><div class="authors">Lukas Höllein; Ang Cao; Andrew Owens; Justin Johnson; Matthias Niessner</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2303.11989.pdf" data-type="Paper">Paper</a> <a href="posters/iccv23_poster_text2room.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We present Text2Room, a method for generating room-scale textured 3D meshes from a given text prompt as input. To this end, we leverage pre-trained 2D text-to-image models to synthesize a sequence of images from different poses. In order to lift these outputs into a consistent 3D scene representation, we combine monocular depth estimation with a text-conditioned inpainting model. The core idea of our approach is a tailored viewpoint selection such that the content of each image can be fused into a seamless, textured 3D mesh. More specifically, we propose a continuous alignment strategy that iteratively fuses scene frames with the existing geometry to create a seamless mesh. Unlike existing works that focus on generating single objects [56, 41] or zoom-out trajectories [18] from text, our method generates complete 3D scenes with multiple objects and explicit 3D geometry. We evaluate our approach using qualitative and quantitative metrics, demonstrating it as the first method to generate room-scale 3D geometry with compelling textures from only text as input.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://minjung-s.github.io/ballgan">BallGAN: 3D-aware Image Synthesis with a Spherical Background</a></div><div class="authors">Minjung Shin; Yunji Seo; Jeongmin Bae; Young Sun Choi; Hyunsu Kim; Hyeran Byun; Youngjung Uh</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2301.09091.pdf" data-type="Paper">Paper</a> <a href="posters/ballgan_workshop.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">3D-aware GANs aim to synthesize realistic 3D scenes that can be rendered in arbitrary camera viewpoints, generating high-quality images with well-defined geometry. As 3D content creation becomes more popular, the ability to generate foreground objects separately from the background has become a crucial property. Existing methods have been developed regarding overall image quality, but they can not generate foreground objects only and often show degraded 3D geometry. In this work, we propose to represent the background as a spherical surface for multiple reasons inspired by computer graphics. Our method naturally provides foreground-only 3D synthesis facilitating easier 3D content creation. Furthermore, it improves the foreground geometry of 3D-aware GANs and the training stability on datasets with complex backgrounds.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://blandocs.github.io/blendnerf">3D-aware Blending with Generative NeRFs</a></div><div class="authors">Hyunsu Kim; Gayoung Lee; Yunjey Choi; Jin-Hwa Kim; Jun-Yan Zhu</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2302.06608.pdf" data-type="Paper">Paper</a> <a href="posters/blendnerf_poster_workshop.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Image blending aims to combine multiple images seamlessly. It remains challenging for existing 2D-based methods, especially when input images are misaligned due to differences in 3D camera poses and object shapes. To tackle these issues, we propose a 3D-aware blending method using generative Neural Radiance Fields (NeRF), including two key components: 3D-aware alignment and 3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of the reference image with respect to generative NeRFs and then perform pose alignment for objects. To further leverage 3D information of the generative NeRF, we propose 3D-aware blending that utilizes volume density and blends on the NeRF's latent space, rather than raw pixel space.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://cap3d-um.github.io/">Scalable 3D Captioning with Pretrained Models</a></div><div class="authors">Tiange Luo; Chris Rockwell; Honglak Lee; Justin Johnson</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2306.07279.pdf" data-type="Paper">Paper</a> <a href="posters/Cap3D_poster.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point·E, Shap·E, and DreamFusion.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://salad3d.github.io/">SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation</a></div><div class="authors">Juil Koo; Seungwoo Yoo; Hieu Minh Nguyen; Minhyuk Sung</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2303.12236.pdf" data-type="Paper">Paper</a> <a href="posters/SALAD-portrait.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We present a cascaded diffusion model based on a part-level implicit 3D representation. Our model achieves state-of-the-art generation quality and also enables part-level shape editing and manipulation without any additional training in conditional setup. Diffusion models have demonstrated impressive capabilities in data generation as well as zero-shot completion and editing via a guided reverse process. Recent research on 3D diffusion models has focused on improving their generation capabilities with various data representations, while the absence of structural information has limited their capability in completion and editing tasks. We thus propose our novel diffusion model using a part-level implicit representation. To effectively learn diffusion with high-dimensional embedding vectors of parts, we propose a cascaded framework, learning diffusion first on a low-dimensional subspace encoding extrinsic parameters of parts and then on the other high-dimensional subspace encoding intrinsic attributes. In the experiments, we demonstrate the outperformance of our method compared with the previous ones both in generation and part-level completion and manipulation tasks.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://www.obukhov.ai/repainting_3d_assets">Breathing New Life into 3D Assets with Generative Repainting</a></div><div class="authors">Tianfu Wang; Menelaos Kanakis; Konrad Schindler; Luc Van Gool; Anton Obukhov</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://www.obukhov.ai/pdf/paper_repainting_3d_assets.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">Diffusion-based text-to-image models ignited immense attention from the vision community, artists, and content creators. Broad adoption of these models is due to significant improvement in the quality of generations and efficient conditioning on various modalities, not just text. However, lifting the rich generative priors of these 2D models into 3D is challenging. Recent works have proposed various pipelines powered by the entanglement of diffusion models and neural fields. We explore the power of pretrained 2D diffusion models and standard 3D neural radiance fields as independent, standalone tools and demonstrate their ability to work together in a non-learned fashion. Such modularity has the intrinsic advantage of eased partial upgrades, which became an important property in such a fast-paced domain. Our pipeline accepts any legacy renderable geometry, such as textured or untextured meshes, orchestrates the interaction between 2D generative refinement and 3D consistency enforcement tools, and outputs a painted input geometry in several formats. We conduct a large-scale study on a wide range of objects and categories from the ShapeNetSem dataset and demonstrate the advantages of our approach, both qualitatively and quantitatively.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://github.com/threestudio-project/threestudio">threestudio: a modular framework for diffusion-guided 3D generation</a></div><div class="authors">Ying-Tian Liu; Yuan-Chen Guo; Vikram Voleti; Ruizhi Shao; Chia-Hao Chen; Guan Luo; Zixin Zou; Chen Wang; Christian Laforte; Yan-Pei Cao; Song-Hai Zhang</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://cg.cs.tsinghua.edu.cn/threestudio/ICCV2023_AI3DCC_threestudio.pdf" data-type="Paper">Paper</a> <a href="posters/threestudio_poster.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We introduce threestudio, an open-source, unified, and modular framework specifically designed for 3D content generation. This framework extends diffusion-based 2D image generation models to 3D generation guidance while incorporating conditions such as text and images. We delineate the modular architecture and design of each component within threestudio. Moreover, we re-implement state-of-the-art methods for 3D generation within threestudio, presenting comprehensive comparisons of their design choices. This versatile framework has the potential to empower researchers and developers to delve into cutting-edge techniques for 3D generation, and presents the capability to facilitate further applications beyond 3D generation.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://farm3d.github.io/">Learning Articulated 3D Animals by Distilling 2D Diffusion</a></div><div class="authors">Tomas Jakab; Ruining Li; Shangzhe Wu; Christian Rupprecht; Andrea Vedaldi</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2304.10535.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">We present Farm3D, a method to learn category-specific 3D reconstructors for articulated objects entirely from 'free' virtual supervision from a pre-trained 2D diffusion-based image generator. Recent approaches can learn, given a collection of single-view images of an object category, a monocular network to predict the 3D shape, albedo, illumination and viewpoint of any object occurrence. We propose a framework using an image generator like Stable Diffusion to generate virtual training data for learning such a reconstruction network from scratch. Furthermore, we include the diffusion model as a score to further improve learning. The idea is to randomise some aspects of the reconstruction, such as viewpoint and illumination, generating synthetic views of the reconstructed 3D object, and have the 2D network assess the quality of the resulting image, providing feedback to the reconstructor. Different from work based on distillation which produces a single 3D asset for each textual prompt in hours, our approach produces a monocular reconstruction network that can output a controllable 3D asset from a given image, real or generated, in only seconds. Our network can be used for analysis, including monocular reconstruction, or for synthesis, generating articulated assets for real-time applications such as video games.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://meshdiffusion.github.io/">MeshDiffusion: Score-based Generative 3D Mesh Modeling</a></div><div class="authors">Zhen Liu; Yao Feng; Michael J. Black; Derek Nowrouzezahrai; Liam Paull; Weiyang Liu</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2303.08133.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://taeksuu.github.io/ncho/">NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</a></div><div class="authors">Taeksoo Kim; Shunsuke Saito; Hanbyul Joo</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2305.14345.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">Deep generative models have been recently extended to synthesizing 3D digital humans. However, previous approaches treat clothed humans as a single chunk of geometry without considering the compositionality of clothing and accessories. As a result, individual items cannot be naturally composed into novel identities, leading to limited expressiveness and controllability of generative 3D avatars. While several methods attempt to address this by leveraging synthetic data, the interaction between humans and objects is not authentic due to the domain gap, and manual asset creation is difficult to scale for a wide variety of objects. In this work, we present a novel framework for learning a compositional generative model of humans and objects (backpacks, coats, scarves, and more) from real-world 3D scans. Our compositional model is interaction-aware, meaning the spatial relationship between humans and objects, and the mutual shape change by physical contact is fully incorporated. The key challenge is that, since humans and objects are in contact, their 3D scans are merged into a single piece. To decompose them without manual annotations, we propose to leverage two sets of 3D scans of a single person with and without objects. Our approach learns to decompose objects and naturally compose them back into a generative human model in an unsupervised manner. Despite our simple setup requiring only the capture of a single subject with objects, our experiments demonstrate the strong generalization of our model by enabling the natural composition of objects to diverse identities in various poses and the composition of multiple objects, which is unseen in training data.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://snuvclab.github.io/chupa/">Chupa : Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models</a></div><div class="authors">Byungjun Kim; Patrick Kwon; Kwangho Lee; Myunggi Lee; Sookwan Han; Daesik Kim; Hanbyul Joo</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2305.11870.pdf" data-type="Paper">Paper</a> <a href="posters/chupa_poster.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We propose a 3D generation pipeline that uses diffusion models to generate realistic human digital avatars. Due to the wide variety of human identities, poses, and stochastic details, the generation of 3D human meshes has been a challenging problem. To address this, we decompose the problem into 2D normal map generation and normal map-based 3D reconstruction. Specifically, we first simultaneously generate realistic normal maps for the front and backside of a clothed human, dubbed dual normal maps, using a pose-conditional diffusion model. For 3D reconstruction, we ``carve'' the prior SMPL-X mesh to a detailed 3D mesh according to the normal maps through mesh optimization. To further enhance the high-frequency details, we present a diffusion resampling scheme on both body and facial regions, thus encouraging the generation of realistic digital avatars. We also seamlessly incorporate a recent text-to-image diffusion model to support text-based human identity control. Our method, namely, Chupa, is capable of generating realistic 3D clothed humans with better perceptual quality and identity variety.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2309.07668">CoRF : Colorizing Radiance Fields using Knowledge Distillation</a></div><div class="authors">Ankit Dhiman; Srinath R; SrinjaySoumitra Sarkar; Lokesh Boregowda; Venkatesh Babu RADHAKRISHNAN</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2309.07668.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">Neural radiance field (NeRF) based methods enable high-quality novel-view synthesis for multi-view images. This work presents a method for synthesizing colorized novel views from input grey-scale multi-view images. When we apply image or video-based colorization methods on the generated grey-scale novel views, we observe artifacts due to inconsistency across views. Training a radiance field network on the colorized grey-scale image sequence also does not solve the 3D consistency issue. We propose a distillation based method to transfer color knowledge from the colorization networks trained on natural images to the radiance field network. Specifically, our method uses the radiance field network as a 3D representation and transfers knowledge from existing 2D colorization methods. The experimental results demonstrate that the proposed method produces superior colorized novel views for indoor and outdoor scenes while maintaining cross-view consistency than baselines. Further, we show the efficacy of our method on applications like colorization of radiance field network trained from 1.) Infra-Red (IR) multi-view images and 2.) Old grey-scale multi-view image sequences.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://sherwinbahmani.github.io/cc3d/">CC3D: Layout-Conditioned Generation of Compositional 3D Scenes</a></div><div class="authors">Sherwin Bahmani; Jeong Joon Park; Despoina Paschalidou; Xingguang Yan; Gordon Wetzstein; Leonidas Guibas; Andrea Tagliasacchi</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2303.12074.pdf" data-type="Paper">Paper</a> <a href="posters/cc3d_poster_iccv3.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">Recent years have seen significant progress in training 3D-aware image generators from unstructured image collections. In this work, we introduce CC3D, a conditional generative model that synthesizes complex 3D scenes conditioned on 2D semantic scene layouts. Different from most existing 3D GANs that limit their applicability to aligned single objects, we focus on generating complex 3D scenes with multiple objects, by modeling the compositional nature of 3D scenes. By devising a 2D layout-based approach for 3D synthesis and implementing a new 3D field representation with a stronger geometric inductive bias, we have created a 3D-GAN that is both efficient and of high quality, while allowing for a more controllable generation process. Our evaluations on synthetic 3D-FRONT and real-world KITTI-360 datasets demonstrate that our model generates scenes of improved visual and geometric quality in comparison to previous works.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2307.03869">Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation</a></div><div class="authors">Aditya Sanghi ; Pradeep Kumar Jayaraman; Arianna Rampini; Joseph G Lambourne; Hooman Shayani; Evan Atherton; Saeid Asgari Taghanaki</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2307.03869.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">Significant progress has been made in using large pre-trained models for creative applications in 3D vision, such as text-to-shape generation. However, generating 3D shapes from sketches remains challenging due to the limited availability of paired datasets linking sketches to corresponding 3D shapes, as well as variations in sketch abstraction levels. To address this challenge, we propose a solution: conditioning a 3D generative model on features extracted from a frozen pre-trained vision model, specifically using features obtained from synthetic renderings during training. This approach enables the effective generation of 3D shapes from sketches at inference time, demonstrating that the pre-trained model features carry semantic signals that are resilient to domain shifts.In our experiments, we validate the effectiveness of our method by showing that it can generate multiple 3D shapes per input sketch, regardless of the sketch's level of abstraction. Importantly, our technique achieves this without the need for paired datasets during training.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://jiyewise.github.io/projects/LAMA/">Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments</a></div><div class="authors">Jiye Lee; Hanbyul Joo</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2301.02667.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">Synthesizing interaction-involved human motions has been challenging due to the high complexity of 3D environments and the diversity of possible human behaviors within. We present LAMA, Locomotion-Action-MAnipulation, to synthesize natural and plausible long term human movements in complex indoor environments. The key motivation of LAMA is to build a unified framework to encompass a series of everyday motions including locomotion, scene interaction, and object manipulation. Unlike existing methods that require motion data ''paired'' with scanned 3D scenes for supervision, we formulate the problem as a test-time optimization by using human motion capture data only for synthesis. LAMA leverages a reinforcement learning framework coupled with motion matching algorithm for optimization, and further exploits a motion editing framework via manifold learning to cover possible variations in interaction and manipulation. Throughout extensive experiments, we demonstrate that LAMA outperforms previous approaches in synthesizing realistic motions in various challenging scenarios.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://scenediffuser.github.io/">Diffusion-based Generation, Optimization, and Planning in 3D Scenes</a></div><div class="authors">Siyuan Huang; Zan Wang; Puhao Li; Baoxiong Jia; Tengyu Liu; Yixin Zhu; Wei Liang; Song-Chun Zhu</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2301.06015.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">We introduce the SceneDiffuser, a conditional generative model for 3D scene understanding. SceneDiffuser provides a unified model for solving scene-conditioned generation, optimization, and planning. In contrast to prior work, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy, SceneDiffuser jointly formulates the scene-aware generation, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differentiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previous scene-conditioned generative models. We evaluate the SceneDiffuser on various 3D scene understanding tasks, including human pose and motion generation, dexterous grasp generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant improvements compared with previous models, demonstrating the tremendous potential of the SceneDiffuser for the broad community of 3D scene understanding.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="">Text-driven Human Avatar Generation by Neural Re-parameterized Texture Optimization</a></div><div class="authors">Kim Youwang; Tae-Hyun Oh</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="/papers/0042_camready.pdf" data-type="Paper">Paper</a> <a href="posters/0042_poster.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">We present TexAvatar, a text-driven human texture generation system for creative human avatar synthesis. Despite the huge progress in text-driven human avatar generation methods, modeling high-quality, efficient human appearance remains challenging. With our proposed neural re-parameterized texture optimization, TexAvatar generates a high-quality UV texture in 30 minutes, given only a text description. The generated UV texture can be easily superimposed on animatable human meshes without further processing. This is distinctive in that prior works generate volumetric textured avatars that require cumbersome rigging processes to animate. We demonstrate that TexAvatar produces human avatars with favorable quality, with faster speed, compared to recent competing methods.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://sirui-xu.github.io/InterDiff/">InterDiff: Forecasting 3D Human-Object Interaction with Physics-Informed Diffusion</a></div><div class="authors">Sirui Xu; Zhengyuan Li; Yu-Xiong Wang; Liangyan Gui</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2308.16905.pdf" data-type="Paper">Paper</a><div class="link-content" data-index="0">This paper addresses the task of anticipating 3D human-object interactions (HOIs). Previous research has either ignored object dynamics or lacked whole-body interaction being limited to grasping small objects. Our task is challenging, as it requires modeling dynamic objects with various shapes, whole-body motion, and ensuring physically valid interaction. To this end, we propose InterDiff, a framework comprising two key steps: (i) interaction diffusion, where we leverage a diffusion model to capture the distribution of future human-object interactions; (ii) interaction correction, where we introduce a physics-informed predictor via coordinate transformations to correct for denoised HOIs in a diffusion step. Our key insight is to inject prior knowledge that the interactions under reference with respect to the contact points follow a simple pattern and are easily predictable. Experiments on large-scale human motion datasets demonstrate the effectiveness of our method for the new task, capable of producing realistic, vivid, and remarkably long-term 3D HOI predictions.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="https://arxiv.org/abs/2309.10684">Locally Stylized Neural Radiance Fields</a></div><div class="authors">Hong Wing Pang; Binh-Son Hua; Sai-Kit Yeung</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="https://arxiv.org/pdf/2309.10684.pdf" data-type="Paper">Paper</a> <a href="posters/nerf_stylization_poster.pdf" data-type="Poster">Poster</a><div class="link-content" data-index="0">In recent years, there has been increasing interest in applying stylization on 3D scenes from a reference style image, in particular onto neural radiance fields (NeRF). While performing stylization directly on NeRF guarantees appearance consistency over arbitrary novel views, it is a challenging problem to guide the transfer of patterns from the style image onto different parts of the NeRF scene. In this work, we propose a stylization framework for NeRF based on local style transfer. In particular, we use a hash-grid encoding to learn the embedding of the appearance and geometry components, and show that the mapping defined by the hash table allows us to control the stylization to a certain extent. Stylization is then achieved by optimizing the appearance branch while keeping the geometry branch fixed. To support local style transfer, we propose a new loss function that utilizes a segmentation network and bipartite matching to establish region correspondences between the style image and the content images obtained from volume rendering. Our experiments show that our method yields plausible stylization results with novel view synthesis while having flexible controllability via manipulating and customizing the region correspondences.</div></div></div></div><div class="row paper"><div class="content"><div class="paper-title"><a href="#">ContactGen: Generative Contact Modeling for Grasp Generation</a></div><div class="authors">Shaowei Liu; Yang Zhou; Jimei Yang</div><div class="links"><a href="#" data-type="Abstract" data-index="0">Abstract</a> <a href="#" data-type="Paper">Paper</a><div class="link-content" data-index="0">This paper presents a novel object-centric contact representation ContactGen for hand-object interaction. The ContactGen comprises 3 components: a contact map indicates the contact location, a part map represents the contact hand part, and a direction map tells the contact direction within each part. Given an input object, we propose a conditional generative model to predict ContactGen and adopt model-based optimization to predict diverse and geometrically feasible grasps. Experimental results demonstrate our method can generate high-fidelity and diverse human grasps for various objects.</div></div></div></div><!-- end non-arc paper list -->
    
        <div class="d-flex" id="footer">
            <div class="col-6 d-none d-md-block col-button" style="text-align: left;">
            </div>
            <div class="col-6 d-none d-md-block col-button" style="text-align: right;">
                <a class="btn btn-primary me-md-2" role="button" href="#home" >Top</a>
            </div>
        </div>
        <!---
        <div class="d-grid gap-2 d-md-flex justify-content-md-end d-none d-md-block col-button">
            <a class="btn btn-primary me-md-2" role="button" href="#home" >Top</a>
        </div>-->

        <script>
            var scrollSpy = new bootstrap.ScrollSpy(document.body, {
                target: '#navbar-top'
            });
        </script>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-4LXC9BQBBX"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
        
          gtag('config', 'G-4LXC9BQBBX');
        </script>

        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }
                    console.log(ev);

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>
    </body>
</html>

